{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3288a8d-41a1-4b19-8821-686894424a71",
   "metadata": {},
   "source": [
    "# Topic modeling (LDA)\n",
    "- It implements latent Dirichlet allocation (a popular topic modeling approach)\n",
    "- The model uses collapsed Gibbs sampling (a faster inference model for topic modeling)\n",
    "\n",
    "It operates in two steps.\n",
    "\n",
    "*A) Preparing data (integer encoding documents)*  \n",
    "\n",
    "*B) Performing topic modeling on integer encoded documents*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f238531-1daf-4776-a790-1e0281e6c37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's a vanilla implementation of Topic modeling that only uses basic tools:\n",
    "# json - to read from and write to files in JSON format \n",
    "# numpy - for faster matrix operations \n",
    "# pandas - to read CSV data\n",
    "# string - to only keep English letters, removing punctuation and other characters\n",
    "# random - to generate random numbers for initializing Markov-chain Monte Carlo, and \n",
    "#           and during the algorithm's working to avoid local optima\n",
    "\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9e85cc-1cbb-4730-9b2d-7b5fa8e3cdaf",
   "metadata": {},
   "source": [
    "# A) Preparing data (integer encoding documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abb420d-bbf4-4c45-9863-8dff51c3848f",
   "metadata": {},
   "source": [
    "1. Read textual data\n",
    "2. Generate integer encoding\n",
    "3. Storing intermediate data\n",
    "\n",
    "**Working with integers (representing words or unique tokens is much faster than the word strings itself)**\n",
    "\n",
    "*At the end, the integers would be reversed back to their respective words*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce624119-ed66-40bc-831b-69bfcf4bd0de",
   "metadata": {},
   "source": [
    "## 1. Reading textual data\n",
    "- Read raw text from a .txt file, having a document per line\n",
    "- Separate into a list of documents\n",
    "- Tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac322afb-36d2-4c35-98c5-856f652d5bf3",
   "metadata": {},
   "source": [
    "1.1 Clean text by removing punctuations and characters other than English letters \\\n",
    "1.2 Convert to lower case \\\n",
    "1.3 Tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f363163-e010-4d32-8c3e-f3d580886247",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "\n",
    "def clean_text(text):\n",
    "    clean_text = text.lower()\n",
    "    # cleaning documents by removing unwanted characters\n",
    "    clean_text = \"\".join([char for char in text if char in string.ascii_lowercase])\n",
    "    # cleaning documents by stopwords\n",
    "    clean_text = [word for word in text.split(\" \") if word not in en_stopwords and len(word) > 2]\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92385f09-70e1-4976-8ed3-8553fd996cdb",
   "metadata": {},
   "source": [
    "1.4 Read data from the file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cc2c232-1965-4fd2-a880-56d6cd9e6ce5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2225"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read input data: titles of BBC articles available on the following link\n",
    "# https://github.com/vahadruya/Capstone-Project-Unsupervised-ML-Topic-Modelling/blob/main/Input_Data/input.csv\n",
    "\n",
    "with open('config.json', 'r') as file:\n",
    "    config = json.load(file)\n",
    "\n",
    "with open(config[\"input-path\"], 'r') as file:\n",
    "    input_data = file.read().split('\\n')\n",
    "\n",
    "\n",
    "\n",
    "# Tokenize sentences into words\n",
    "tokenized_documents = []\n",
    "for document in input_data: # Considering only the first 100 titles for the sake of demonstration\n",
    "    document = clean_text(document)\n",
    "    tokenized_documents.append(document)\n",
    "len(tokenized_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc667374-bf14-4907-8230-354aa6235ec2",
   "metadata": {},
   "source": [
    "## 2. Configuration Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57be5529-ce14-47b5-834e-30fee7263b0f",
   "metadata": {},
   "source": [
    "`config.json`\n",
    "\n",
    "The method provides the following configuration options to alter the behavior of the method.\n",
    "\n",
    "**numTopics:** Number of topics to extract from the dataset. The default value is 3. However, it generally depends on the nature of the data. Keeping the number of topics too few can only have the topics focused on broader concepts, and cannot identify the specific topics. While keeping the number of topics too high results in noisy (incoherent) topics. Therefore, a suitable number of topics depends on the data and the type of analysis required. The default value here is 3.\n",
    "\n",
    "**numAlpha**: This hyperparameter helps in deciding the probabilities of topics in a document. A higher value (above 1) will have too many topics with similar probabilities, i.e., when the intended purpose is to get more topics per document. However, keeping the value too low will have only a few prominent topics with very high probabilities as compared to others. A lower value (below 1) is used when fewer topics are needed per document. A lower numAlpha value pushes higher probabilities higher and lower probabilities further lower. While a higher numAlpha value introduces a high bias, due to which all probabilities converge to similar values. In this method, we are using the numAlpha value 1.\n",
    "\n",
    "**numBeta**: This hyperparameter helps in deciding the probabilities of words in a topic. a higher value (above 1) will have too many words with similar probabilities, i.e., when the intended purpose is to have more words representing a topic. However, keeping the value too low will have only a few most prominent words with very high probabilities as compared to others. Generally, considering the size of the vocabulary in a dataset, this value is kept smaller to determine only the most relevant words. In this method, we are using the numBeta value 0.01.\n",
    "\n",
    "**numGSIterations**: It's the number of iterations of the inference technique (collapsed Gibbs sampling). Due to random initialization, more words switch their topics in the earlier iterations, which keeps dropping in the coming iterations, i.e., approaching the equilibrium state. Keeping the number of numGSIterations higher ensures that the words have settled down in their respective topics. Alternatively, the difference between two consecutive iterations is also used to avoid unnecessary iterations when the words have already settled. In this method, we only use the numGSIterations with a value of 1000.\n",
    "\n",
    "**wordsPerTopic**: The number of top words to represent a topic. In this method, we are using the 10 most prominent words for each topic. Due to polysemy, a word may exist in different topics with different neighboring words highlighting its context. \n",
    "\n",
    "**text-doc-path**: path of input (raw text) file\n",
    "**integer-encoded-doc-path**: path of integer integer-encoded file. It is the intermediate file that topic modeling uses. \n",
    "**integer-word-dict**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ea9ce6-e97d-4e02-a211-5f599afc3f72",
   "metadata": {},
   "source": [
    "## 3. Generate Integer encoding\n",
    "It preserves both frequency and position-related information. The process involves assigning each unique token a dedicated integer id, preserving it in a dictionary for later retrieval, while rewriting documents by replacing with with their integer ids.\n",
    "\n",
    "It makes the operations a lot faster as numbers are much faster to read/store and compare as compared to strings. \n",
    "\n",
    "The integer IDs will be replaced with their original words at the end using the stored dictionary files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012edbf6-18fe-4044-add9-b5f175baa0e3",
   "metadata": {},
   "source": [
    "3.1 Generate integer encoded documents \\\n",
    "3.2 Generate word-integer index and integer index-word dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e324e9f6-b624-4853-9a81-695ff3f1eafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of unique tokens and assign integers\n",
    "dictionary = {}\n",
    "revdictionary = {}\n",
    "index = 0\n",
    "\n",
    "#tokenized_documents = [[word for word in doc if word not in esw] for doc in tokenized_documents]\n",
    "\n",
    "for doc in tokenized_documents:\n",
    "    for word in doc:\n",
    "        if word not in dictionary.keys():\n",
    "            dictionary[word] = index\n",
    "            revdictionary[index] = word\n",
    "            index += 1\n",
    "\n",
    "# Replace words in sentences with their corresponding integers\n",
    "encoded_documents = [[dictionary[word] for word in doc] for doc in tokenized_documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05d5e66-58d1-44bd-9ef4-34281d186423",
   "metadata": {},
   "source": [
    "## 4. Storing intermediate data\n",
    "The integer-encoded documents are stored in files\n",
    "The word-to-id and id-to-word dictionaries are also stored\n",
    "\n",
    "*It will help to avoid these steps each time topic modeling is performed under different settings*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "525e3caf-ef52-4ac3-9577-7a77795b0655",
   "metadata": {},
   "outputs": [],
   "source": [
    "toStr = ''\n",
    "for endoc in encoded_documents:\n",
    "    toStr = toStr + '\\t'.join(str(item) for item in endoc)\n",
    "    toStr = toStr + '\\n'\n",
    "toStr = toStr[:-2]\n",
    "file = open('data/integer-encoded-data.txt', 'w')\n",
    "file.write(toStr)\n",
    "file.close()\n",
    "\n",
    "#write dictionary to file\n",
    "file = open('data/dictionary.json', 'w')\n",
    "file.write(json.dumps(dictionary))\n",
    "file.close()\n",
    "file = open('data/revdictionary.json', 'w')\n",
    "file.write(json.dumps(revdictionary))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7122c2b-d37a-4159-9e30-8070dc3af78b",
   "metadata": {},
   "source": [
    "# B) Topic Modeling\n",
    "- It identifies the hidden thematic structures within the documents and represents them as latent topics.\n",
    "- Each document is a mixture of all possible topics with varying probabilities\n",
    "- Each topic is a mixture of all the vocabulary of the dataset with varying probabilities\n",
    "- This method implements Latent Dirichlet allocation (LDA), a commonly used topic model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8a9a10-9280-4df0-9812-77084b1879c1",
   "metadata": {},
   "source": [
    "*Setting random seeds*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c350497-fe5e-4216-b04f-e41dd29289bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reproducible results\n",
    "random.seed(41)  # For Python random\n",
    "np.random.seed(41)  # For NumPy random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fef4c3a-7e99-42da-87b7-bb665e34d514",
   "metadata": {},
   "source": [
    "## 1. Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17229a3-21b5-4656-8a1c-0733ca137a26",
   "metadata": {},
   "source": [
    "**LDA class**\n",
    "The main functions are:\n",
    "1. Random initialization (assigning word occurrences to topics at random)\n",
    "2. Using Markov Chain Monte Carlo (MCMC) sampling, the posterior distribution is estimated using the current state (converging by iterations)\n",
    "3. Collapsed Gibbs sampling inference: in each iteration \\\n",
    "   3.1 Iterates through all documents, all tokens/words in each document \\\n",
    "   3.2 For each token, compute its most suitable topic, given the current status of the model \\\n",
    "   3.3 Updates the new topic if different from the  current topic, associated estimates update, so does the model state \\\n",
    "4. Estimate document-topic distribution from the final state of the model \n",
    "5. Estimate topic-word distribution (organized in decreasing order of probabilities) from the final state of the model\n",
    "6. Other utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d77b49ce-7684-40ac-9f15-1184d6887b4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The class implements topic modeling (Latent dirichlet allocation) algorithm using collapsed gibbs sampling as in inference. \n",
    "class LDA:\n",
    "    # topics to extract from the data (Components)\n",
    "    _numTopics = None\n",
    "    # vocabulary (unique words) in the dataset\n",
    "    _arrVocab = None\n",
    "    #size of vocabulary (count of unique words)\n",
    "    _numVocabSize = None\n",
    "    # dataset\n",
    "    _arrDocs = []\n",
    "    # dataset size (number of documents)\n",
    "    _numDocSize = None\n",
    "    # dirichlet prior (document to topic prior)\n",
    "    _numAlpha = None\n",
    "    # dirichlet prior (topic to word prior)\n",
    "    _numBeta = None\n",
    "    _ifScalarHyperParameters = True\n",
    "    # Gibb sampler iterations\n",
    "    _numGSIterations = None\n",
    "    # The iterations for initial burnin (update of parameters)\n",
    "    _numNBurnin = None\n",
    "    # The iterations for continuous burnin (update of parameters)\n",
    "    _numSampleLag = None\n",
    "    \n",
    "    \n",
    "    \n",
    "    # The following attributes are for internal working\n",
    "    __numTAlpha = None  \n",
    "    __numVBeta = None   \n",
    "    __arrTheta = None\n",
    "    __arrThetaSum = None\n",
    "    __arrPhi = None\n",
    "    __arrPhiSum = None\n",
    "    __arrNDT = None\n",
    "    __arrNDSum = []\n",
    "    __arrNTW = None\n",
    "    __arrNTSum = []\n",
    "    __arrZ = []\n",
    "    \n",
    "    # for alpha to be a list, its size must be equal to the size of the dataset, and has a value for each doc\n",
    "    # for beta to be a list, its size must be equal to the number of topics, and has a value for each topic  \n",
    "    def __init__(self, numTopics = 2, numAlpha = 1.0, numBeta = 0.01, \n",
    "                 numGSIterations = 1000, numNBurnin = 50, numSampleLag = 20, \n",
    "                 wordsPerTopic = 10):\n",
    "        self._numTopics = config[\"numTopics\"]\n",
    "        self._numAlpha = config[\"numAlpha\"]\n",
    "        self._numBeta = config[\"numBeta\"]\n",
    "        self._numGSIterations = config[\"numGSIterations\"]\n",
    "        self._numNBurni = config[\"numNBurnin\"]\n",
    "        self._numSampleLag = config[\"numSampleLag\"]\n",
    "        self.__wordsPerTopic = config[\"wordsPerTopic\"]\n",
    "            \n",
    "    #load data as integer encoding of words in a sequence (no padding or truncation)\n",
    "    def getData(self, path):\n",
    "        file = open(path, 'r')\n",
    "        rawData = file.read()\n",
    "        file.close()\n",
    "        self.__loadData(rawData)\n",
    "        self.__loadVocab()\n",
    "        self.__prepareCollections()\n",
    "\n",
    "    #load docs and docSize from the dataset\n",
    "    def __loadData(self, rawData):\n",
    "        rows = rawData.split('\\n')\n",
    "         \n",
    "        #read dataset as documents of words IDs\n",
    "        for row in rows:\n",
    "            swordlist = row.split('\\t')\n",
    "            swordlist = list(filter(None, swordlist))   #remove empty items from list\n",
    "            if len(swordlist) > 0:\n",
    "                iwordlist = [eval(w) for w in swordlist]    \n",
    "                self._arrDocs.append(iwordlist)\n",
    "\n",
    "        # determine dataset size\n",
    "        self._numDocSize = len(self._arrDocs)\n",
    "        \n",
    "        \n",
    "    #Determine unique words (vocabulary) and count of unique words (vocabSize)    \n",
    "    def __loadVocab(self):\n",
    "        #determine unique vocabulary\n",
    "        uniqueWords = []\n",
    "        for doc in self._arrDocs:\n",
    "            for word in doc:\n",
    "                if word not in uniqueWords:\n",
    "                    uniqueWords.append(word)\n",
    "        self._arrVocab = uniqueWords\n",
    "        self._numVocabSize = len(self._arrVocab)    \n",
    "\n",
    "    def __prepareCollections(self):\n",
    "        self.__arrNDSum = np.array([0] * self._numDocSize)\n",
    "        self.__arrTheta = np.array([[0] * self._numTopics] * self._numDocSize)\n",
    "        self.__arrThetasum = np.array([[0] * self._numTopics] * self._numDocSize)\n",
    "        self.__arrNDT = np.array([[0] * self._numTopics] * self._numDocSize)\n",
    "        \n",
    "        self.__arrNTSum = np.array([0] * self._numTopics)\n",
    "        self.__arrPhi = np.array([[0] * self._numVocabSize] * self._numTopics)\n",
    "        self.__arrPhisum = np.array([[0] * self._numVocabSize] * self._numTopics)\n",
    "        self.__arrNTW = np.array([[0] * self._numVocabSize] * self._numTopics)\n",
    "\n",
    "        #Assign values to parameters based on hyper-parameters\n",
    "        self.__numTAlpha = self._numTopics*self._numAlpha  \n",
    "        self.__numVBeta = self._numVocabSize*self._numBeta   \n",
    "\n",
    "        \n",
    "        for d in range(0, self._numDocSize):\n",
    "            rowOfZeros = [0] * len(self._arrDocs[d])\n",
    "            self.__arrZ.append(rowOfZeros)\n",
    "                \n",
    "    # Initialize first markov chain randomly\n",
    "    def randomMarkovChainInitialization(self):\n",
    "        \n",
    "        for d in range(self._numDocSize):\n",
    "            wta = []                        #wta - word topic assignment\n",
    "            doc = self._arrDocs[d]\n",
    "            for ind in range(len(doc)): \n",
    "                randtopic = random.randint(0, self._numTopics - 1)      # generate a topic number at random\n",
    "                self.__arrZ[d][ind] = randtopic\n",
    "                self.__arrNDT[d][randtopic] += 1\n",
    "                self.__arrNDSum[d] += 1\n",
    "                wordid = self._arrDocs[d][ind]\n",
    "                self.__arrNTW[randtopic][wordid] += 1\n",
    "                self.__arrNTSum[randtopic] += 1\n",
    "            \n",
    "    \n",
    "    #Inference (Collapsed Gibbs Sampling)\n",
    "    def gibbsSampling(self):\n",
    "        tAlpha = self._numAlpha * self._numTopics\n",
    "        vBeta = self._numBeta * self._numVocabSize            \n",
    "                    \n",
    "        for it in range(self._numGSIterations):\n",
    "            for d in range(self._numDocSize):\n",
    "                dsize = len(self._arrDocs[d])\n",
    "                for ind in range(dsize):\n",
    "                    # remove old topic from a word instance\n",
    "                    oldTopic = self.__arrZ[d][ind]\n",
    "                    wordid = self._arrDocs[d][ind]\n",
    "                    self.__arrNDT[d][oldTopic] -= 1\n",
    "                    self.__arrNDSum[d] -= 1\n",
    "                    self.__arrNTW[oldTopic][wordid] -= 1\n",
    "                    self.__arrNTSum[oldTopic] -= 1   \n",
    "\n",
    "                    # find a new more appropriate tpoic for the word instanc as per current state of the model\n",
    "                    prob = [0] * self._numTopics\n",
    "                    \n",
    "                    for t in range(self._numTopics):\n",
    "                        prob[t] = ((self.__arrNDT[d][t] + self._numAlpha) / (self.__arrNDSum[d] + tAlpha)) * \\\n",
    "                            (self.__arrNTW[t][wordid] + self._numBeta) / (self.__arrNTSum[t] + vBeta)\n",
    "                    \n",
    "                    #cumulate multinomial\n",
    "                    cdf = prob\n",
    "                    for x in range(1, len(cdf)):\n",
    "                        cdf[x] += cdf[x-1]\n",
    "                    \n",
    "                    cutoff = random.random() * cdf[-1]\n",
    "                    newTopic = 0\n",
    "                    for i in range(len(cdf)):\n",
    "                        if cdf[i] > cutoff:\n",
    "                            newTopic = i\n",
    "                            break\n",
    "                    #update as per new topic\n",
    "                    self.__arrZ[d][ind] = newTopic\n",
    "                    self.__arrNDT[d][newTopic] += 1\n",
    "                    self.__arrNDSum[d] += 1\n",
    "                    self.__arrNTW[newTopic][wordid] += 1\n",
    "                    self.__arrNTSum[newTopic] += 1\n",
    "                \n",
    "    def getTopicsPerDocument(self):\n",
    "        dtd = {}\n",
    "        for d in range(self._numDocSize):\n",
    "            for t in range(self._numTopics):\n",
    "                val = (self.__arrNDT[d][t]+self._numAlpha)/(self.__arrNDSum[d]+self.__numTAlpha)\n",
    "                val = round(val, 4)\n",
    "                key = \"topic-\" + str(t+1)\n",
    "                if key not in dtd.keys():\n",
    "                    dtd[key] = []\n",
    "                dtd[key].append(val)\n",
    "        return dtd\n",
    "\n",
    "    def getWordsPerTopic(self, revdictionary):\n",
    "        twd = []\n",
    "        for t in range(self._numTopics):\n",
    "            wpt = {}\n",
    "            for v in range(self._numVocabSize):\n",
    "                val = (self.__arrNTW[t][v]+self._numBeta)/(self.__arrNTSum[t]+self.__numVBeta)\n",
    "                val = round(val, 4)\n",
    "                wpt[revdictionary[str(v)]] = val\n",
    "             #   flag += 1\n",
    "             #   if flag == self.__wordsPerTopic:\n",
    "             #       break\n",
    "            wpt = sorted(wpt.items(), key=lambda x: x[1], reverse=True)[:self.__wordsPerTopic]\n",
    "            twd.append(wpt)\n",
    "        output = {}\n",
    "        for i in range(len(twd)):\n",
    "            output[\"topic-\" + str(i+1)] = twd[i]\n",
    "        return output\n",
    "    \n",
    "    def printall(self):\n",
    "        print(\"topics: \", self._numTopics)\n",
    "        print(\"dataset: \", self._arrDocs)\n",
    "        print(\"dataset size: \", self._numDocSize)\n",
    "        print(\"vocab: \", self._arrVocab)\n",
    "        print(\"vocab size: \", self._numVocabSize)\n",
    "        print(\"ndt: \", self.__arrNDT)\n",
    "        print(\"ndsum: \", self.__arrNDSum)\n",
    "        print(\"ntw: \", self.__arrNTW)\n",
    "        print(\"ntsum: \", self.__arrNTSum)\n",
    "        print(\"z: \", self.__arrZ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d99a1e5-6dfc-4611-a3bc-4520acb9d657",
   "metadata": {},
   "source": [
    "## 2. Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49454d2b-c9a3-4003-9f40-17dddbf36b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    lda = LDA()\n",
    "    lda.getData(config[\"integer-encoded-path\"])\n",
    "    lda.randomMarkovChainInitialization()\n",
    "    lda.gibbsSampling()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8bd5a3-e674-485d-a1cf-090a843d38ef",
   "metadata": {},
   "source": [
    "## 3. Results\n",
    "\n",
    "Topic modeling has two important results\n",
    "1. **Latent topics** identified in the corpus. Each topic is represented by the top-most presentable words for that topic. It's similar to clustering in the sense that the words are grouped as topics and labeled un-intuitively as topic 1, topic 2, etc. However, unlike clustering, the words have probabilities of relevance to the other words of the topic. Using these probabilities, only the top few words (10) are used to represent a topic. Therefore, it is also called *word topic distribution*. \n",
    "\n",
    "2. **Topics in documents** are the probabilities of topics within each document. A general conception is that a document is not entirely about a single topic and instead has different percentages of multiple topics. The topics in documents provide the probabilities of each topic in each document.\n",
    "\n",
    "To observe the output generated by this method closely, please go to the `data/output-data/` folder, which has `word-topic-distribution.txt` and `document-topic-distribution.txt` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1806691-8039-4174-aaf5-498a9fcd2d46",
   "metadata": {},
   "source": [
    "*Topic word distribution*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71cf8f2-ae9d-458f-bd03-742ec71a16ce",
   "metadata": {},
   "source": [
    "**Topic distribution per document** \\\n",
    "Each document talks about the topics identified to a different extent. For example, document 1 is 12.5% topic 1, 37.5% topic 2, and 50% topic 3. \n",
    "\n",
    "Readers interested in a specific topic may only read the documents where that topic has high coverage. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4dc42807-a4c0-4316-aed8-2870c6358859",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic-1</th>\n",
       "      <th>topic-2</th>\n",
       "      <th>topic-3</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Doc 1</th>\n",
       "      <td>0.3750</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>India calls for fair trade rules</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc 2</th>\n",
       "      <td>0.3750</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>Sluggish economy hits German jobs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc 3</th>\n",
       "      <td>0.3750</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>Indonesians face fuel price rise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc 4</th>\n",
       "      <td>0.3750</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>Court rejects $280bn tobacco case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc 5</th>\n",
       "      <td>0.1429</td>\n",
       "      <td>0.2857</td>\n",
       "      <td>0.5714</td>\n",
       "      <td>Dollar gains on Greenspan speech</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       topic-1  topic-2  topic-3                               Text\n",
       "Doc 1   0.3750   0.2500   0.3750   India calls for fair trade rules\n",
       "Doc 2   0.3750   0.2500   0.3750  Sluggish economy hits German jobs\n",
       "Doc 3   0.3750   0.2500   0.3750   Indonesians face fuel price rise\n",
       "Doc 4   0.3750   0.3750   0.2500  Court rejects $280bn tobacco case\n",
       "Doc 5   0.1429   0.2857   0.5714   Dollar gains on Greenspan speech"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "doc_topic_dist = lda.getTopicsPerDocument()\n",
    "doc_topic_dist[\"Text\"] = input_data\n",
    "\n",
    "# Printing topic distribution for the top 10 documents\n",
    "df_dtd = pd.DataFrame(doc_topic_dist, index = [\"Doc \"+str(i+1) for i in range(len(doc_topic_dist['topic-1']))])\n",
    "\n",
    "df_dtd.to_csv(\"data/output-data/document-topic-distribution.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "df_dtd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db038df4-c2dd-41c3-aa79-6686c84909e1",
   "metadata": {},
   "source": [
    "**words distribution per topic** \\\n",
    "The three latent topics determined from this dataset are labeled as Topic 1, Topic 2, and Topic 3. \\\n",
    "*Topic 1:* May be broadly interpreted as election and politics  from its top words \\\n",
    "*Topic 2:* May be broadly interpreted as business deals \\\n",
    "*Topic 3:* May be broadly interpreted as movies and showbiz\n",
    "\n",
    "These three topics give a general idea of the topics covered in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "59102eea-eb42-41c9-bbf9-9a392ee05a90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic-1</th>\n",
       "      <th>topic-2</th>\n",
       "      <th>topic-3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>word 1</th>\n",
       "      <td>(new, 0.0158)</td>\n",
       "      <td>(win, 0.009)</td>\n",
       "      <td>(film, 0.0123)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word 2</th>\n",
       "      <td>(Blair, 0.0136)</td>\n",
       "      <td>(deal, 0.0087)</td>\n",
       "      <td>(set, 0.0089)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word 3</th>\n",
       "      <td>(hits, 0.0096)</td>\n",
       "      <td>(show, 0.0087)</td>\n",
       "      <td>(top, 0.008)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word 4</th>\n",
       "      <td>(net, 0.009)</td>\n",
       "      <td>(shares, 0.0068)</td>\n",
       "      <td>(hit, 0.0077)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word 5</th>\n",
       "      <td>(election, 0.0071)</td>\n",
       "      <td>(plan, 0.0068)</td>\n",
       "      <td>(wins, 0.0074)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word 6</th>\n",
       "      <td>(Labour, 0.0071)</td>\n",
       "      <td>(firm, 0.0065)</td>\n",
       "      <td>(return, 0.0071)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word 7</th>\n",
       "      <td>(growth, 0.0065)</td>\n",
       "      <td>(China, 0.0065)</td>\n",
       "      <td>(bid, 0.0071)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word 8</th>\n",
       "      <td>(face, 0.0062)</td>\n",
       "      <td>(back, 0.0065)</td>\n",
       "      <td>(gets, 0.0065)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word 9</th>\n",
       "      <td>(says, 0.0062)</td>\n",
       "      <td>(takes, 0.0065)</td>\n",
       "      <td>(Brown, 0.0065)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word 10</th>\n",
       "      <td>(row, 0.0062)</td>\n",
       "      <td>(Yukos, 0.0062)</td>\n",
       "      <td>(economy, 0.0061)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    topic-1           topic-2            topic-3\n",
       "word 1        (new, 0.0158)      (win, 0.009)     (film, 0.0123)\n",
       "word 2      (Blair, 0.0136)    (deal, 0.0087)      (set, 0.0089)\n",
       "word 3       (hits, 0.0096)    (show, 0.0087)       (top, 0.008)\n",
       "word 4         (net, 0.009)  (shares, 0.0068)      (hit, 0.0077)\n",
       "word 5   (election, 0.0071)    (plan, 0.0068)     (wins, 0.0074)\n",
       "word 6     (Labour, 0.0071)    (firm, 0.0065)   (return, 0.0071)\n",
       "word 7     (growth, 0.0065)   (China, 0.0065)      (bid, 0.0071)\n",
       "word 8       (face, 0.0062)    (back, 0.0065)     (gets, 0.0065)\n",
       "word 9       (says, 0.0062)   (takes, 0.0065)    (Brown, 0.0065)\n",
       "word 10       (row, 0.0062)   (Yukos, 0.0062)  (economy, 0.0061)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(config[\"integer-word-dict-path\"], 'r') as file:\n",
    "    revdictionary = json.load(file)\n",
    "topic_word_distribution = lda.getWordsPerTopic(revdictionary)\n",
    "\n",
    "df_twd = pd.DataFrame(topic_word_distribution, index = [\"word \" +str(i+1) for i in range(len(topic_word_distribution['topic-1']))])\n",
    "\n",
    "df_twd.to_csv(\"data/output-data/topic-word-distribution.tsv\", sep=\"\\t\", index = False)\n",
    "df_twd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cc7c59-b386-40be-b1b5-0b361fdda0a1",
   "metadata": {},
   "source": [
    "*Document topic distribution*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d50a05-c34c-45f0-9ec4-a38a65a7651d",
   "metadata": {},
   "source": [
    "*print all details:*\n",
    "- Integer encoded dataset\n",
    "- Final state of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31e55f81-a968-4fd2-8f55-7e4dca76564c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prints everything - for debugging\n",
    "#lda.printall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5784e48f-03da-4155-b671-05e4c8087e34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
