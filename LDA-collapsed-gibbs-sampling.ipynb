{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9908102",
   "metadata": {},
   "source": [
    "# Key Concepts and Definitions\n",
    "\n",
    "This heading clearly introduces the definitions and explanations provided in the following section.\n",
    "Before we dive into the notebook, let's take a moment to break down a few key concepts in a simple and friendly way. This will help make the rest of the material much easier to follow!\n",
    "\n",
    "**What does \"Latent\" mean?**  \n",
    "In data science, \"latent\" means something that is hidden or not directly visible. For example, in text analysis, there might be hidden themes or patterns in a collection of documents that are not obvious just by reading them. These hidden patterns are called \"latent\" because they are not directly observed but can be discovered using special methods.\n",
    "\n",
    "**What is a \"Topic\"?**  \n",
    "A \"topic\" is a group of words that often appear together and represent a common theme or subject in a set of documents. For example, words like \"economy,\" \"growth,\" and \"production\" might form a topic about economics. In topic modeling, the computer tries to find these groups of words (topics) that best describe the themes in the documents.\n",
    "\n",
    "**What is \"Sampling\"?**  \n",
    "\"Sampling\" is a way to make guesses or estimates by looking at a small part of the data instead of the whole thing. In topic modeling, sampling is used to repeatedly guess which topic a word belongs to, based on the current state of the model. Over many rounds, these guesses help the model discover the hidden topics in the documents. Think of it like drawing names from a hat many times to get a good idea of what names are in the hat.\n",
    "\n",
    "**What is \"Gibbs Sampling\"?**  \n",
    "Gibbs sampling is a special kind of sampling used to estimate hidden patterns when there are many unknowns. In topic modeling, Gibbs sampling works by going through each word in each document and guessing which topic it belongs to, based on the current guesses for all the other words. After each guess, the model updates its understanding of the topics. By repeating this process many times, the model gradually gets better at figuring out which words belong to which topics. It's like solving a big puzzle by making lots of small, smart guesses, one piece at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2ea4ad",
   "metadata": {},
   "source": [
    "## Topic modeling (LDA)\n",
    "- It implements latent dirichlet allocation (a popular topic modeling approach)\n",
    "- The model uses collapsed gibbs sampling (a faster inference model for topic modeling)\n",
    "\n",
    "It operates in two steps.\n",
    "\n",
    "*A) Preparing data (integer encoding documents)*  \n",
    "\n",
    "*B) Performing topic modeling on integer encoded documents*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1226449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Its vanilla implementation of Topic modeling that only uses basic tools:\n",
    "# json - to read from and write to files in json format \n",
    "# numpy - for faster matrix operations \n",
    "# pandas - to read csv data\n",
    "# string - to only keep English letters, removing puntuations and other characters\n",
    "# random - to generate random numbers for initializing Markov-chain monte carlo, and \n",
    "#           and during algorithm working to avoid local optima\n",
    "\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9e85cc-1cbb-4730-9b2d-7b5fa8e3cdaf",
   "metadata": {},
   "source": [
    "# A) Preparing data (integer encoding documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abb420d-bbf4-4c45-9863-8dff51c3848f",
   "metadata": {},
   "source": [
    "1. Read textual data\n",
    "2. Generate integer encoding\n",
    "3. Storing intemediate data\n",
    "\n",
    "**Working with integers (representing words or unique tokens is much faster than the word strings itself)**\n",
    "\n",
    "*At the end, the integers would be reversed back to their respective words*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce624119-ed66-40bc-831b-69bfcf4bd0de",
   "metadata": {},
   "source": [
    "## 1. Reading textual data\n",
    "- Read raw text from .txt file having document per line\n",
    "- Separate into list of documents\n",
    "- Tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac322afb-36d2-4c35-98c5-856f652d5bf3",
   "metadata": {},
   "source": [
    "1.1 Clean text by removing punctuations and characters othen than English letters \\\n",
    "1.2 Convert to lower case \\\n",
    "1.3 Tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f363163-e010-4d32-8c3e-f3d580886247",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "\n",
    "def clean_text(text):\n",
    "    clean_text = text.lower()\n",
    "    # cleaning documents by removing unwanted characters\n",
    "    clean_text = \"\".join([char for char in text if char in string.ascii_lowercase])\n",
    "    # cleaning documents by stopwords\n",
    "    clean_text = [word for word in text.split(\" \") if word not in en_stopwords and len(word) > 2]\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92385f09-70e1-4976-8ed3-8553fd996cdb",
   "metadata": {},
   "source": [
    "1.4 Read data from the file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cc2c232-1965-4fd2-a880-56d6cd9e6ce5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read input data: titles of BBC articles available on the following link\n",
    "# https://github.com/vahadruya/Capstone-Project-Unsupervised-ML-Topic-Modelling/blob/main/Input_Data/input.csv\n",
    "\n",
    "with open('config.json', 'r') as file:\n",
    "    config = json.load(file)\n",
    "\n",
    "with open(config[\"input-path\"], 'r') as file:\n",
    "    input_data = file.read().split('\\n')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Tokenize sentences into words\n",
    "tokenized_documents = []\n",
    "for document in input_data[:100]: # Considering only first 100 titles for the sake of demonstration\n",
    "    document = clean_text(document)\n",
    "    if len(document) > 2:\n",
    "        tokenized_documents.append(document)\n",
    "len(tokenized_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc667374-bf14-4907-8230-354aa6235ec2",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57be5529-ce14-47b5-834e-30fee7263b0f",
   "metadata": {},
   "source": [
    "In the repository you can find `config.json`. The method provides the following configuration options to alter the behavior of the method from `config.json`\n",
    "\n",
    "**numTopics:** Number of topics to extract from the dataset. The default value is 3. However, it generally depends on the nature of the data. Keeping the number of topics too few can only have the topics focused on broader concepts and cannot identify the specific topics. While keeping the number of topics too high results in noisy (incoherent) topics. Therefore, a suitable number of topics depends on the data and the type of analysis required. The default value here is 3.\n",
    "\n",
    "**numAlpha**: This hyperparameter helps in deciding the probabilities of topics in a document. A higher value (above 1) will have too many topics with similar probabilities i.e., when the intended purpose is to get more topics per document. However, keeping the value too low will have only few promiment topics with very high probabilities as compared to others. A lower value (below 1) is used when only fewer topics are needed per document. A lower numAlpha value pushes higher probabilities higher and lower probabilities further lower. While a higher numAlpha value introduces a high bias due to which all probabilities converges to similar values. In this method we are using numAlpha value 1.\n",
    "\n",
    "**numBeta**: This hyperparameter helps in deciding the probabilities of words in a topic. a higher value (above 1) will have too many words with similar probabilities i.e., when the intended purpose is to have more words representing a topic. However, keeping the value too low will have only few most prominent words with very high probabilities as compared to others. Generally, considering the size of vocabulary in a dataset, this value is kept smaller to determine only the most relevant words. In this method we are using numBeta value 0.01.\n",
    "\n",
    "**numGSIterations**: Its the number of iterations of the inference technique (collapsed Gibbs sampling). Due to random initialization, there are more words switching their topics in the earlier iterations that keeps dropping in the coming iterations i.e., approaching the equilibrium state. Keeping the number of numGSIterations higher ensures that the words have settled down in their respective topics. Alternately, the difference between two consecutive iterations is also used to avoid unnecessary iterations when the words have already settled. In this method we only use the numGSIterations with value 1000.\n",
    "\n",
    "**wordsPerTopic**: The number of top words to represent a topic. In this method we are using 10 most prominent words for each topic. Due to polysemy, it is possible that a word exist in different topics with different neighboring words highlighting its context. \n",
    "\n",
    "**text-doc-path**: path of input (raw text) file\n",
    "**integer-encoded-doc-path**: path of integer encoded file. It is the intermediate file that topic modeling use. \n",
    "**integer-word-dict**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ea9ce6-e97d-4e02-a211-5f599afc3f72",
   "metadata": {},
   "source": [
    "## 3. Generate Integer encoding\n",
    "It preserves both frequency and position related information. The process involves assigning each unique token a dedicated integer id, preserving it in a dictionary for later retrieval, while rewriting documents by replacing with with their integer ids.\n",
    "\n",
    "It makes the operations a lot faster as numbers are much faster to read/store and compare as compared to strings. \n",
    "\n",
    "The integer ids will be replaced with their original words at the end using stored dictionary files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012edbf6-18fe-4044-add9-b5f175baa0e3",
   "metadata": {},
   "source": [
    "3.1 Generate integer encoded documents \\\n",
    "3.2 Generate word-integer index and integer index-word dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e324e9f6-b624-4853-9a81-695ff3f1eafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of unique tokens and assign integers\n",
    "dictionary = {}\n",
    "revdictionary = {}\n",
    "index = 0\n",
    "\n",
    "#tokenized_documents = [[word for word in doc if word not in esw] for doc in tokenized_documents]\n",
    "\n",
    "for doc in tokenized_documents:\n",
    "    for word in doc:\n",
    "        if word not in dictionary.keys():\n",
    "            dictionary[word] = index\n",
    "            revdictionary[index] = word\n",
    "            index += 1\n",
    "\n",
    "# Replace words in sentences with their corresponding integers\n",
    "encoded_documents = [[dictionary[word] for word in doc] for doc in tokenized_documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05d5e66-58d1-44bd-9ef4-34281d186423",
   "metadata": {},
   "source": [
    "### 4. Storing intermediate data\n",
    "The integer encoded documents are stored in files\n",
    "the word-to-id and id-to-word dictionaries are also stored\n",
    "\n",
    "*It will help to avoid these steps, each time topic modeling is performed under different settings*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "525e3caf-ef52-4ac3-9577-7a77795b0655",
   "metadata": {},
   "outputs": [],
   "source": [
    "toStr = ''\n",
    "for endoc in encoded_documents:\n",
    "    toStr = toStr + '\\t'.join(str(item) for item in endoc)\n",
    "    toStr = toStr + '\\n'\n",
    "toStr = toStr[:-2]\n",
    "file = open('data/integer-encoded-data.txt', 'w')\n",
    "file.write(toStr)\n",
    "file.close()\n",
    "\n",
    "#write dictionary to file\n",
    "file = open('data/dictionary.json', 'w')\n",
    "file.write(json.dumps(dictionary))\n",
    "file.close()\n",
    "file = open('data/revdictionary.json', 'w')\n",
    "file.write(json.dumps(revdictionary))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7122c2b-d37a-4159-9e30-8070dc3af78b",
   "metadata": {},
   "source": [
    "# B) Topic Modeling (LDA)\n",
    "- It identifies the hidden thematic structures within the documents and represent them as latent topics.\n",
    "- Each document is a mixture of all possible topics with varying probabilities\n",
    "- Each topic is a mixture of all vocabulary of the dataset with varying probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8a9a10-9280-4df0-9812-77084b1879c1",
   "metadata": {},
   "source": [
    "*Setting random seeds*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c350497-fe5e-4216-b04f-e41dd29289bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reproducible results\n",
    "random.seed(41)  # For Python random\n",
    "np.random.seed(41)  # For NumPy random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17229a3-21b5-4656-8a1c-0733ca137a26",
   "metadata": {},
   "source": [
    "## 1. Latent Dirichlet Allocation (LDA)\n",
    "**LDA class**\n",
    "main functions are:\n",
    "1. Markov chain monte carlo initialization (giving the model a random inital state, expecting the model\n",
    "    to converge for higher number of iterations.\n",
    "2. Collapsed gibbs sampling inference: in each iteration \\\n",
    "   2.1 Iterates through all documents, all tokens/words in each document \\\n",
    "   2.2 For for each token computes its most suitable topic, given the current status of the model \\\n",
    "   2.3 Updates new topic if different from current topic, associated estimates update, so does the model state \\\n",
    "3. Estimate document-topic distribution from the final state of the model \n",
    "4. Estimate topic-word distribution (organized in decreasing order of probabilities) from the final state of the model\n",
    "5. Other utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d77b49ce-7684-40ac-9f15-1184d6887b4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The class implements topic modeling (Latent dirichlet allocation) algorithm using collapsed gibbs sampling as in inference. \n",
    "class LDA:\n",
    "    # topics to extract from the data (Components)\n",
    "    _numTopics = None\n",
    "    # vocabulary (unique words) in the dataset\n",
    "    _arrVocab = None\n",
    "    #size of vocabulary (count of unique words)\n",
    "    _numVocabSize = None\n",
    "    # dataset\n",
    "    _arrDocs = []\n",
    "    # dataset size (number of documents)\n",
    "    _numDocSize = None\n",
    "    # dirichlet prior (document to topic prior)\n",
    "    _numAlpha = None\n",
    "    # dirichlet prior (topic to word prior)\n",
    "    _numBeta = None\n",
    "    _ifScalarHyperParameters = True\n",
    "    # Gibb sampler iterations\n",
    "    _numGSIterations = None\n",
    "    # The iterations for initial burnin (update of parameters)\n",
    "    _numNBurnin = None\n",
    "    # The iterations for continuous burnin (update of parameters)\n",
    "    _numSampleLag = None\n",
    "    \n",
    "    \n",
    "    \n",
    "    # The following attributes are for internal working\n",
    "    __numTAlpha = None  \n",
    "    __numVBeta = None   \n",
    "    __arrTheta = None\n",
    "    __arrThetaSum = None\n",
    "    __arrPhi = None\n",
    "    __arrPhiSum = None\n",
    "    __arrNDT = None\n",
    "    __arrNDSum = []\n",
    "    __arrNTW = None\n",
    "    __arrNTSum = []\n",
    "    __arrZ = []\n",
    "    \n",
    "    # for alpha to be a list, its size must be equal to the size of the dataset, has value for each doc\n",
    "    # for beta to be a list, its size must be equal to the number of topics, has value for each topic  \n",
    "    def __init__(self, numTopics = 2, numAlpha = 1.0, numBeta = 0.01, \n",
    "                 numGSIterations = 1000, numNBurnin = 50, numSampleLag = 20, \n",
    "                 wordsPerTopic = 10):\n",
    "        self._numTopics = config[\"numTopics\"]\n",
    "        self._numAlpha = config[\"numAlpha\"]\n",
    "        self._numBeta = config[\"numBeta\"]\n",
    "        self._numGSIterations = config[\"numGSIterations\"]\n",
    "        self._numNBurnin = config[\"numNBurnin\"]\n",
    "        self._numSampleLag = config[\"numSampleLag\"]\n",
    "        self.__wordsPerTopic = config[\"wordsPerTopic\"]\n",
    "            \n",
    "    #load data as integer encoding of words in a sequence (no padding or truncation)\n",
    "    def getData(self, path):\n",
    "        file = open(path, 'r')\n",
    "        input_data = file.read()\n",
    "        file.close()\n",
    "        self.__loadData(input_data)\n",
    "        self.__loadVocab()\n",
    "        self.__prepareCollections()\n",
    "\n",
    "    #load docs and docSize from the dataset\n",
    "    def __loadData(self, input_data):\n",
    "        rows = input_data.split('\\n')\n",
    "         \n",
    "        #read dataset as documents of words IDs\n",
    "        for row in rows:\n",
    "            swordlist = row.split('\\t')\n",
    "            swordlist = list(filter(None, swordlist))   #remove empty items from list\n",
    "            if len(swordlist) > 0:\n",
    "                iwordlist = [eval(w) for w in swordlist]    \n",
    "                self._arrDocs.append(iwordlist)\n",
    "\n",
    "        # determine dataset size\n",
    "        self._numDocSize = len(self._arrDocs)\n",
    "        \n",
    "        \n",
    "    #Determine unique words (vocabulary) and count of unique words (vocabSize)    \n",
    "    def __loadVocab(self):\n",
    "        #determine unique vocabulary\n",
    "        uniqueWords = []\n",
    "        for doc in self._arrDocs:\n",
    "            for word in doc:\n",
    "                if word not in uniqueWords:\n",
    "                    uniqueWords.append(word)\n",
    "        self._arrVocab = uniqueWords\n",
    "        self._numVocabSize = len(self._arrVocab)    \n",
    "\n",
    "    def __prepareCollections(self):\n",
    "        self.__arrNDSum = np.array([0] * self._numDocSize)\n",
    "        self.__arrTheta = np.array([[0] * self._numTopics] * self._numDocSize)\n",
    "        self.__arrThetasum = np.array([[0] * self._numTopics] * self._numDocSize)\n",
    "        self.__arrNDT = np.array([[0] * self._numTopics] * self._numDocSize)\n",
    "        \n",
    "        self.__arrNTSum = np.array([0] * self._numTopics)\n",
    "        self.__arrPhi = np.array([[0] * self._numVocabSize] * self._numTopics)\n",
    "        self.__arrPhisum = np.array([[0] * self._numVocabSize] * self._numTopics)\n",
    "        self.__arrNTW = np.array([[0] * self._numVocabSize] * self._numTopics)\n",
    "\n",
    "        #Assign values to parameters based on hyper-parameters\n",
    "        self.__numTAlpha = self._numTopics*self._numAlpha  \n",
    "        self.__numVBeta = self._numVocabSize*self._numBeta   \n",
    "\n",
    "        \n",
    "        for d in range(0, self._numDocSize):\n",
    "            rowOfZeros = [0] * len(self._arrDocs[d])\n",
    "            self.__arrZ.append(rowOfZeros)\n",
    "                \n",
    "    # Initialize first markov chain randomly\n",
    "    def randomMarkovChainInitialization(self):\n",
    "        \n",
    "        for d in range(self._numDocSize):\n",
    "            wta = []                        #wta - word topic assignment\n",
    "            doc = self._arrDocs[d]\n",
    "            for ind in range(len(doc)): \n",
    "                randtopic = random.randint(0, self._numTopics - 1)      # generate a topic number at random\n",
    "                self.__arrZ[d][ind] = randtopic\n",
    "                self.__arrNDT[d][randtopic] += 1\n",
    "                self.__arrNDSum[d] += 1\n",
    "                wordid = self._arrDocs[d][ind]\n",
    "                self.__arrNTW[randtopic][wordid] += 1\n",
    "                self.__arrNTSum[randtopic] += 1\n",
    "            \n",
    "    \n",
    "    #Inference (Collapsed Gibbs Sampling)\n",
    "    def gibbsSampling(self):\n",
    "        tAlpha = self._numAlpha * self._numTopics\n",
    "        vBeta = self._numBeta * self._numVocabSize            \n",
    "                    \n",
    "        for it in range(self._numGSIterations):\n",
    "            for d in range(self._numDocSize):\n",
    "                dsize = len(self._arrDocs[d])\n",
    "                for ind in range(dsize):\n",
    "                    # remove old topic from a word instance\n",
    "                    oldTopic = self.__arrZ[d][ind]\n",
    "                    wordid = self._arrDocs[d][ind]\n",
    "                    self.__arrNDT[d][oldTopic] -= 1\n",
    "                    self.__arrNDSum[d] -= 1\n",
    "                    self.__arrNTW[oldTopic][wordid] -= 1\n",
    "                    self.__arrNTSum[oldTopic] -= 1   \n",
    "\n",
    "                    # find a new more appropriate tpoic for the word instanc as per current state of the model\n",
    "                    prob = [0] * self._numTopics\n",
    "                    \n",
    "                    for t in range(self._numTopics):\n",
    "                        prob[t] = ((self.__arrNDT[d][t] + self._numAlpha) / (self.__arrNDSum[d] + tAlpha)) * \\\n",
    "                            (self.__arrNTW[t][wordid] + self._numBeta) / (self.__arrNTSum[t] + vBeta)\n",
    "                    \n",
    "                    #cumulate multinomial\n",
    "                    cdf = prob\n",
    "                    for x in range(1, len(cdf)):\n",
    "                        cdf[x] += cdf[x-1]\n",
    "                    \n",
    "                    cutoff = random.random() * cdf[-1]\n",
    "                    newTopic = 0\n",
    "                    for i in range(len(cdf)):\n",
    "                        if cdf[i] > cutoff:\n",
    "                            newTopic = i\n",
    "                            break\n",
    "                    #update as per new topic\n",
    "                    self.__arrZ[d][ind] = newTopic\n",
    "                    self.__arrNDT[d][newTopic] += 1\n",
    "                    self.__arrNDSum[d] += 1\n",
    "                    self.__arrNTW[newTopic][wordid] += 1\n",
    "                    self.__arrNTSum[newTopic] += 1\n",
    "                \n",
    "    def getTopicsPerDocument(self):\n",
    "        results = ''\n",
    "        results += \"***Topics per Document***\\n\"\n",
    "        doc_topic_dist = {f\"topic-{t}\": [] for t in range(self._numTopics)}\n",
    "        for d in range(self._numDocSize):\n",
    "            results += \"Document \" + str(d) + \":\\n\"\n",
    "            for t in range(self._numTopics):\n",
    "                val = (self.__arrNDT[d][t]+self._numAlpha)/(self.__arrNDSum[d]+self.__numTAlpha)\n",
    "                results += \"Topic \" + str(t) + \":\" + str(val) + '\\t'\n",
    "                doc_topic_dist[f\"topic-{t}\"].append(val)\n",
    "            results += '\\n'\n",
    "        print(results)\n",
    "        file = open('data/output-data/document-topic-distribution.txt', 'w')\n",
    "        file.write(results)\n",
    "        return doc_topic_dist\n",
    "    \n",
    "    def getWordsPerTopic(self, revdictionary):\n",
    "        results = \"***Words per Topic***\\n\"\n",
    "        topic_word_dist = {}\n",
    "        for t in range(self._numTopics):\n",
    "            results += \"\\nTopic \" + str(t) + \":\"\n",
    "            #flag = 0\n",
    "            wpt = {}\n",
    "            for v in range(self._numVocabSize):\n",
    "                val = (self.__arrNTW[t][v]+self._numBeta)/(self.__arrNTSum[t]+self.__numVBeta)\n",
    "                wpt[revdictionary[str(v)]] = float(val)\n",
    "             #   flag += 1\n",
    "             #   if flag == self.__wordsPerTopic:\n",
    "             #       break\n",
    "            results += '\\n'\n",
    "            wpt_sorted = sorted(wpt.items(), key=lambda x: x[1], reverse=True)[:self.__wordsPerTopic]\n",
    "            topic_word_dist[f\"topic-{t}\"] = wpt_sorted\n",
    "            for item in wpt_sorted:\n",
    "                results += str(item)\n",
    "        file = open('data/output-data/topic-word-distribution.txt', 'w')\n",
    "        file.write(results)\n",
    "        print(results)\n",
    "        return topic_word_dist\n",
    "    \n",
    "    def printall(self):\n",
    "        print(\"topics: \", self._numTopics)\n",
    "        print(\"dataset: \", self._arrDocs)\n",
    "        print(\"dataset size: \", self._numDocSize)\n",
    "        print(\"vocab: \", self._arrVocab)\n",
    "        print(\"vocab size: \", self._numVocabSize)\n",
    "        print(\"ndt: \", self.__arrNDT)\n",
    "        print(\"ndsum: \", self.__arrNDSum)\n",
    "        print(\"ntw: \", self.__arrNTW)\n",
    "        print(\"ntsum: \", self.__arrNTSum)\n",
    "        print(\"z: \", self.__arrZ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d99a1e5-6dfc-4611-a3bc-4520acb9d657",
   "metadata": {},
   "source": [
    "## 2. Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49454d2b-c9a3-4003-9f40-17dddbf36b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    lda = LDA()\n",
    "    lda.getData(config[\"integer-encoded-path\"])\n",
    "    lda.randomMarkovChainInitialization()\n",
    "    lda.gibbsSampling()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8bd5a3-e674-485d-a1cf-090a843d38ef",
   "metadata": {},
   "source": [
    "## 3. Results\n",
    "\n",
    "Topic modeling has two important results\n",
    "1. **Latent topics** identified in the corpus. Each topic is represented by top most presentable words for that topic. Its similar to clustering in the sense that the words are grouped as topics and labeled unintuitively as topic 0, topic 1 etc. However, unlike clustering, the words have probabilities of relevance to the other words of the topic. Using these probabilities, only the top few words (10 or 20) are used to represent a topic. Therefore, it is also called *word topic distribution*. \n",
    "\n",
    "2. **Topics in documents** are the probabilities of topics within each document. A general conception is that a document is not about entirely about a single topic and instead has different percentages of multiple topics. The topics in documents provide the probabilities of each topic in each document.\n",
    "\n",
    "To observe the output generated by this method closely, please goto `data/output-data/` folder having `word-topic-distribution.txt` and `document-topic-distribution.txt` \n",
    "\n",
    "**words distribution per topic** \\\n",
    "The three latent topics determind from this dataset are labeled as Topic 0, Topic 1, and Topic 2. \\\n",
    "*Topic 0:* Represents profit, production and growth in economy \\\n",
    "*Topic 1:* Represents trade and and deals of fuel and BMW mentioning Germany and France \\\n",
    "*Topic 2:* Represents jobs, Yukos firm, India and Japan\n",
    "\n",
    "These three topics gives a general idea of the topics covered in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1806691-8039-4174-aaf5-498a9fcd2d46",
   "metadata": {},
   "source": [
    "*Topic word distribution*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea33e09c",
   "metadata": {},
   "source": [
    "**Topic distribution per document** \\\n",
    "Each document talks about the topics identified to different extent. For example, document 0 can be 45% topic 0, 45% topic 1 and 10% topic 2. \n",
    "\n",
    "Therefore, it is important to know which documents are dominated by which topics, so that if a reader is interested in knowing particularly about topic 1 they can only read the documents where topic 1 is the major topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4dc42807-a4c0-4316-aed8-2870c6358859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Topics per Document***\n",
      "Document 0:\n",
      "Topic 0:0.125\tTopic 1:0.5\tTopic 2:0.375\t\n",
      "Document 1:\n",
      "Topic 0:0.375\tTopic 1:0.25\tTopic 2:0.375\t\n",
      "Document 2:\n",
      "Topic 0:0.125\tTopic 1:0.75\tTopic 2:0.125\t\n",
      "Document 3:\n",
      "Topic 0:0.375\tTopic 1:0.25\tTopic 2:0.375\t\n",
      "Document 4:\n",
      "Topic 0:0.7142857142857143\tTopic 1:0.14285714285714285\tTopic 2:0.14285714285714285\t\n",
      "Document 5:\n",
      "Topic 0:0.42857142857142855\tTopic 1:0.2857142857142857\tTopic 2:0.2857142857142857\t\n",
      "Document 6:\n",
      "Topic 0:0.125\tTopic 1:0.375\tTopic 2:0.5\t\n",
      "Document 7:\n",
      "Topic 0:0.25\tTopic 1:0.5\tTopic 2:0.25\t\n",
      "Document 8:\n",
      "Topic 0:0.375\tTopic 1:0.375\tTopic 2:0.25\t\n",
      "Document 9:\n",
      "Topic 0:0.14285714285714285\tTopic 1:0.14285714285714285\tTopic 2:0.7142857142857143\t\n",
      "Document 10:\n",
      "Topic 0:0.1111111111111111\tTopic 1:0.6666666666666666\tTopic 2:0.2222222222222222\t\n",
      "Document 11:\n",
      "Topic 0:0.42857142857142855\tTopic 1:0.2857142857142857\tTopic 2:0.2857142857142857\t\n",
      "Document 12:\n",
      "Topic 0:0.5\tTopic 1:0.375\tTopic 2:0.125\t\n",
      "Document 13:\n",
      "Topic 0:0.4444444444444444\tTopic 1:0.1111111111111111\tTopic 2:0.4444444444444444\t\n",
      "Document 14:\n",
      "Topic 0:0.125\tTopic 1:0.125\tTopic 2:0.75\t\n",
      "Document 15:\n",
      "Topic 0:0.7142857142857143\tTopic 1:0.14285714285714285\tTopic 2:0.14285714285714285\t\n",
      "Document 16:\n",
      "Topic 0:0.375\tTopic 1:0.25\tTopic 2:0.375\t\n",
      "Document 17:\n",
      "Topic 0:0.375\tTopic 1:0.25\tTopic 2:0.375\t\n",
      "Document 18:\n",
      "Topic 0:0.14285714285714285\tTopic 1:0.2857142857142857\tTopic 2:0.5714285714285714\t\n",
      "Document 19:\n",
      "Topic 0:0.5714285714285714\tTopic 1:0.14285714285714285\tTopic 2:0.2857142857142857\t\n",
      "Document 20:\n",
      "Topic 0:0.14285714285714285\tTopic 1:0.42857142857142855\tTopic 2:0.42857142857142855\t\n",
      "Document 21:\n",
      "Topic 0:0.14285714285714285\tTopic 1:0.7142857142857143\tTopic 2:0.14285714285714285\t\n",
      "Document 22:\n",
      "Topic 0:0.2222222222222222\tTopic 1:0.1111111111111111\tTopic 2:0.6666666666666666\t\n",
      "Document 23:\n",
      "Topic 0:0.14285714285714285\tTopic 1:0.14285714285714285\tTopic 2:0.7142857142857143\t\n",
      "Document 24:\n",
      "Topic 0:0.14285714285714285\tTopic 1:0.14285714285714285\tTopic 2:0.7142857142857143\t\n",
      "Document 25:\n",
      "Topic 0:0.5\tTopic 1:0.25\tTopic 2:0.25\t\n",
      "Document 26:\n",
      "Topic 0:0.5\tTopic 1:0.25\tTopic 2:0.25\t\n",
      "Document 27:\n",
      "Topic 0:0.375\tTopic 1:0.125\tTopic 2:0.5\t\n",
      "Document 28:\n",
      "Topic 0:0.2857142857142857\tTopic 1:0.14285714285714285\tTopic 2:0.5714285714285714\t\n",
      "Document 29:\n",
      "Topic 0:0.5714285714285714\tTopic 1:0.14285714285714285\tTopic 2:0.2857142857142857\t\n",
      "Document 30:\n",
      "Topic 0:0.42857142857142855\tTopic 1:0.2857142857142857\tTopic 2:0.2857142857142857\t\n",
      "Document 31:\n",
      "Topic 0:0.375\tTopic 1:0.375\tTopic 2:0.25\t\n",
      "Document 32:\n",
      "Topic 0:0.75\tTopic 1:0.125\tTopic 2:0.125\t\n",
      "Document 33:\n",
      "Topic 0:0.5555555555555556\tTopic 1:0.3333333333333333\tTopic 2:0.1111111111111111\t\n",
      "Document 34:\n",
      "Topic 0:0.2857142857142857\tTopic 1:0.5714285714285714\tTopic 2:0.14285714285714285\t\n",
      "Document 35:\n",
      "Topic 0:0.14285714285714285\tTopic 1:0.5714285714285714\tTopic 2:0.2857142857142857\t\n",
      "Document 36:\n",
      "Topic 0:0.375\tTopic 1:0.5\tTopic 2:0.125\t\n",
      "Document 37:\n",
      "Topic 0:0.25\tTopic 1:0.5\tTopic 2:0.25\t\n",
      "Document 38:\n",
      "Topic 0:0.14285714285714285\tTopic 1:0.14285714285714285\tTopic 2:0.7142857142857143\t\n",
      "Document 39:\n",
      "Topic 0:0.2857142857142857\tTopic 1:0.5714285714285714\tTopic 2:0.14285714285714285\t\n",
      "Document 40:\n",
      "Topic 0:0.14285714285714285\tTopic 1:0.5714285714285714\tTopic 2:0.2857142857142857\t\n",
      "Document 41:\n",
      "Topic 0:0.2857142857142857\tTopic 1:0.42857142857142855\tTopic 2:0.2857142857142857\t\n",
      "Document 42:\n",
      "Topic 0:0.14285714285714285\tTopic 1:0.14285714285714285\tTopic 2:0.7142857142857143\t\n",
      "Document 43:\n",
      "Topic 0:0.375\tTopic 1:0.375\tTopic 2:0.25\t\n",
      "Document 44:\n",
      "Topic 0:0.5\tTopic 1:0.25\tTopic 2:0.25\t\n",
      "Document 45:\n",
      "Topic 0:0.125\tTopic 1:0.5\tTopic 2:0.375\t\n",
      "Document 46:\n",
      "Topic 0:0.25\tTopic 1:0.125\tTopic 2:0.625\t\n",
      "Document 47:\n",
      "Topic 0:0.14285714285714285\tTopic 1:0.2857142857142857\tTopic 2:0.5714285714285714\t\n",
      "Document 48:\n",
      "Topic 0:0.375\tTopic 1:0.125\tTopic 2:0.5\t\n",
      "Document 49:\n",
      "Topic 0:0.14285714285714285\tTopic 1:0.2857142857142857\tTopic 2:0.5714285714285714\t\n",
      "Document 50:\n",
      "Topic 0:0.2857142857142857\tTopic 1:0.5714285714285714\tTopic 2:0.14285714285714285\t\n",
      "Document 51:\n",
      "Topic 0:0.125\tTopic 1:0.5\tTopic 2:0.375\t\n",
      "Document 52:\n",
      "Topic 0:0.2222222222222222\tTopic 1:0.4444444444444444\tTopic 2:0.3333333333333333\t\n",
      "Document 53:\n",
      "Topic 0:0.7142857142857143\tTopic 1:0.14285714285714285\tTopic 2:0.14285714285714285\t\n",
      "Document 54:\n",
      "Topic 0:0.14285714285714285\tTopic 1:0.14285714285714285\tTopic 2:0.7142857142857143\t\n",
      "Document 55:\n",
      "Topic 0:0.5714285714285714\tTopic 1:0.2857142857142857\tTopic 2:0.14285714285714285\t\n",
      "Document 56:\n",
      "Topic 0:0.5\tTopic 1:0.125\tTopic 2:0.375\t\n",
      "Document 57:\n",
      "Topic 0:0.25\tTopic 1:0.625\tTopic 2:0.125\t\n",
      "Document 58:\n",
      "Topic 0:0.2857142857142857\tTopic 1:0.2857142857142857\tTopic 2:0.42857142857142855\t\n",
      "Document 59:\n",
      "Topic 0:0.125\tTopic 1:0.375\tTopic 2:0.5\t\n",
      "Document 60:\n",
      "Topic 0:0.3333333333333333\tTopic 1:0.5555555555555556\tTopic 2:0.1111111111111111\t\n",
      "Document 61:\n",
      "Topic 0:0.16666666666666666\tTopic 1:0.5\tTopic 2:0.3333333333333333\t\n",
      "Document 62:\n",
      "Topic 0:0.2857142857142857\tTopic 1:0.2857142857142857\tTopic 2:0.42857142857142855\t\n",
      "Document 63:\n",
      "Topic 0:0.5714285714285714\tTopic 1:0.2857142857142857\tTopic 2:0.14285714285714285\t\n",
      "Document 64:\n",
      "Topic 0:0.5\tTopic 1:0.125\tTopic 2:0.375\t\n",
      "Document 65:\n",
      "Topic 0:0.2857142857142857\tTopic 1:0.14285714285714285\tTopic 2:0.5714285714285714\t\n",
      "Document 66:\n",
      "Topic 0:0.375\tTopic 1:0.5\tTopic 2:0.125\t\n",
      "Document 67:\n",
      "Topic 0:0.2857142857142857\tTopic 1:0.14285714285714285\tTopic 2:0.5714285714285714\t\n",
      "Document 68:\n",
      "Topic 0:0.14285714285714285\tTopic 1:0.2857142857142857\tTopic 2:0.5714285714285714\t\n",
      "Document 69:\n",
      "Topic 0:0.5\tTopic 1:0.16666666666666666\tTopic 2:0.3333333333333333\t\n",
      "Document 70:\n",
      "Topic 0:0.625\tTopic 1:0.125\tTopic 2:0.25\t\n",
      "Document 71:\n",
      "Topic 0:0.14285714285714285\tTopic 1:0.14285714285714285\tTopic 2:0.7142857142857143\t\n",
      "Document 72:\n",
      "Topic 0:0.16666666666666666\tTopic 1:0.16666666666666666\tTopic 2:0.6666666666666666\t\n",
      "Document 73:\n",
      "Topic 0:0.625\tTopic 1:0.25\tTopic 2:0.125\t\n",
      "Document 74:\n",
      "Topic 0:0.3333333333333333\tTopic 1:0.1111111111111111\tTopic 2:0.5555555555555556\t\n",
      "Document 75:\n",
      "Topic 0:0.42857142857142855\tTopic 1:0.14285714285714285\tTopic 2:0.42857142857142855\t\n",
      "Document 76:\n",
      "Topic 0:0.14285714285714285\tTopic 1:0.7142857142857143\tTopic 2:0.14285714285714285\t\n",
      "Document 77:\n",
      "Topic 0:0.5\tTopic 1:0.375\tTopic 2:0.125\t\n",
      "Document 78:\n",
      "Topic 0:0.7142857142857143\tTopic 1:0.14285714285714285\tTopic 2:0.14285714285714285\t\n",
      "Document 79:\n",
      "Topic 0:0.5714285714285714\tTopic 1:0.2857142857142857\tTopic 2:0.14285714285714285\t\n",
      "Document 80:\n",
      "Topic 0:0.375\tTopic 1:0.25\tTopic 2:0.375\t\n",
      "Document 81:\n",
      "Topic 0:0.125\tTopic 1:0.375\tTopic 2:0.5\t\n",
      "Document 82:\n",
      "Topic 0:0.42857142857142855\tTopic 1:0.2857142857142857\tTopic 2:0.2857142857142857\t\n",
      "Document 83:\n",
      "Topic 0:0.42857142857142855\tTopic 1:0.42857142857142855\tTopic 2:0.14285714285714285\t\n",
      "Document 84:\n",
      "Topic 0:0.42857142857142855\tTopic 1:0.2857142857142857\tTopic 2:0.2857142857142857\t\n",
      "Document 85:\n",
      "Topic 0:0.375\tTopic 1:0.375\tTopic 2:0.25\t\n",
      "Document 86:\n",
      "Topic 0:0.625\tTopic 1:0.125\tTopic 2:0.25\t\n",
      "Document 87:\n",
      "Topic 0:0.16666666666666666\tTopic 1:0.5\tTopic 2:0.3333333333333333\t\n",
      "Document 88:\n",
      "Topic 0:0.42857142857142855\tTopic 1:0.14285714285714285\tTopic 2:0.42857142857142855\t\n",
      "Document 89:\n",
      "Topic 0:0.2857142857142857\tTopic 1:0.5714285714285714\tTopic 2:0.14285714285714285\t\n",
      "Document 90:\n",
      "Topic 0:0.5714285714285714\tTopic 1:0.14285714285714285\tTopic 2:0.2857142857142857\t\n",
      "Document 91:\n",
      "Topic 0:0.14285714285714285\tTopic 1:0.5714285714285714\tTopic 2:0.2857142857142857\t\n",
      "Document 92:\n",
      "Topic 0:0.5\tTopic 1:0.3333333333333333\tTopic 2:0.16666666666666666\t\n",
      "Document 93:\n",
      "Topic 0:0.25\tTopic 1:0.625\tTopic 2:0.125\t\n",
      "Document 94:\n",
      "Topic 0:0.14285714285714285\tTopic 1:0.7142857142857143\tTopic 2:0.14285714285714285\t\n",
      "Document 95:\n",
      "Topic 0:0.25\tTopic 1:0.125\tTopic 2:0.625\t\n",
      "Document 96:\n",
      "Topic 0:0.2857142857142857\tTopic 1:0.14285714285714285\tTopic 2:0.5714285714285714\t\n",
      "Document 97:\n",
      "Topic 0:0.2857142857142857\tTopic 1:0.2857142857142857\tTopic 2:0.42857142857142855\t\n",
      "Document 98:\n",
      "Topic 0:0.25\tTopic 1:0.625\tTopic 2:0.125\t\n",
      "Document 99:\n",
      "Topic 0:0.125\tTopic 1:0.25\tTopic 2:0.625\t\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic-0</th>\n",
       "      <th>topic-1</th>\n",
       "      <th>topic-2</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Doc 1</th>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>India calls for fair trade rules</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc 2</th>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>Sluggish economy hits German jobs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc 3</th>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>Indonesians face fuel price rise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc 4</th>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>Court rejects $280bn tobacco case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc 5</th>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>Dollar gains on Greenspan speech</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        topic-0   topic-1   topic-2                               Text\n",
       "Doc 1  0.125000  0.500000  0.375000   India calls for fair trade rules\n",
       "Doc 2  0.375000  0.250000  0.375000  Sluggish economy hits German jobs\n",
       "Doc 3  0.125000  0.750000  0.125000   Indonesians face fuel price rise\n",
       "Doc 4  0.375000  0.250000  0.375000  Court rejects $280bn tobacco case\n",
       "Doc 5  0.714286  0.142857  0.142857   Dollar gains on Greenspan speech"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_topic_dist = lda.getTopicsPerDocument()\n",
    "doc_topic_dist[\"Text\"] = input_data[:100]\n",
    "\n",
    "# Printing topic distribution for the top 10 documents\n",
    "df_dtd = pd.DataFrame(doc_topic_dist, index = [\"Doc \"+str(i+1) for i in range(len(doc_topic_dist['topic-1']))])\n",
    "\n",
    "df_dtd.to_csv(\"data/output-data/document-topic-distribution.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "df_dtd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fed177",
   "metadata": {},
   "source": [
    "**Word distribution per topic**\n",
    "\n",
    "The three latent topics determined from this dataset are labeled as Topic 1, Topic 2, and Topic 3.\n",
    "\n",
    "Topic 1: May be broadly interpreted as election and politics from its top words\n",
    "\n",
    "Topic 2: May be broadly interpreted as business deals\n",
    "\n",
    "Topic 3: May be broadly interpreted as movies and showbiz\n",
    "\n",
    "These three topics give a general idea of the topics covered in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0bca00f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Words per Topic***\n",
      "\n",
      "Topic 0:\n",
      "('economy', 0.039125056962437336)('profit', 0.03261506412342946)('growth', 0.026105071284421584)('gets', 0.026105071284421584)('sales', 0.019595078445413708)('hits', 0.01308508560640583)('2004', 0.01308508560640583)('loan', 0.01308508560640583)('production', 0.01308508560640583)('high', 0.01308508560640583)\n",
      "Topic 1:\n",
      "('deal', 0.04214290722950704)('fuel', 0.03513077624290021)('German', 0.021106514269686554)('BMW', 0.021106514269686554)('trade', 0.014094383283079725)('rise', 0.014094383283079725)('French', 0.014094383283079725)('record', 0.014094383283079725)('prices', 0.014094383283079725)('boost', 0.014094383283079725)\n",
      "Topic 2:\n",
      "('jobs', 0.024509504309027562)('profits', 0.024509504309027562)('Yukos', 0.024509504309027562)('firm', 0.024509504309027562)('Japan', 0.024509504309027562)('India', 0.01839740847136483)('hits', 0.01839740847136483)('new', 0.01839740847136483)('Japanese', 0.01839740847136483)('$280bn', 0.012285312633702094)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic-0-word</th>\n",
       "      <th>topic-0-word-probability</th>\n",
       "      <th>topic-1-word</th>\n",
       "      <th>topic-1-word-probability</th>\n",
       "      <th>topic-2-word</th>\n",
       "      <th>topic-2-word-probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>word 1</th>\n",
       "      <td>economy</td>\n",
       "      <td>0.039125</td>\n",
       "      <td>deal</td>\n",
       "      <td>0.042143</td>\n",
       "      <td>jobs</td>\n",
       "      <td>0.024510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word 2</th>\n",
       "      <td>profit</td>\n",
       "      <td>0.032615</td>\n",
       "      <td>fuel</td>\n",
       "      <td>0.035131</td>\n",
       "      <td>profits</td>\n",
       "      <td>0.024510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word 3</th>\n",
       "      <td>growth</td>\n",
       "      <td>0.026105</td>\n",
       "      <td>German</td>\n",
       "      <td>0.021107</td>\n",
       "      <td>Yukos</td>\n",
       "      <td>0.024510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word 4</th>\n",
       "      <td>gets</td>\n",
       "      <td>0.026105</td>\n",
       "      <td>BMW</td>\n",
       "      <td>0.021107</td>\n",
       "      <td>firm</td>\n",
       "      <td>0.024510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word 5</th>\n",
       "      <td>sales</td>\n",
       "      <td>0.019595</td>\n",
       "      <td>trade</td>\n",
       "      <td>0.014094</td>\n",
       "      <td>Japan</td>\n",
       "      <td>0.024510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word 6</th>\n",
       "      <td>hits</td>\n",
       "      <td>0.013085</td>\n",
       "      <td>rise</td>\n",
       "      <td>0.014094</td>\n",
       "      <td>India</td>\n",
       "      <td>0.018397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word 7</th>\n",
       "      <td>2004</td>\n",
       "      <td>0.013085</td>\n",
       "      <td>French</td>\n",
       "      <td>0.014094</td>\n",
       "      <td>hits</td>\n",
       "      <td>0.018397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word 8</th>\n",
       "      <td>loan</td>\n",
       "      <td>0.013085</td>\n",
       "      <td>record</td>\n",
       "      <td>0.014094</td>\n",
       "      <td>new</td>\n",
       "      <td>0.018397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word 9</th>\n",
       "      <td>production</td>\n",
       "      <td>0.013085</td>\n",
       "      <td>prices</td>\n",
       "      <td>0.014094</td>\n",
       "      <td>Japanese</td>\n",
       "      <td>0.018397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word 10</th>\n",
       "      <td>high</td>\n",
       "      <td>0.013085</td>\n",
       "      <td>boost</td>\n",
       "      <td>0.014094</td>\n",
       "      <td>$280bn</td>\n",
       "      <td>0.012285</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        topic-0-word  topic-0-word-probability topic-1-word  \\\n",
       "word 1       economy                  0.039125         deal   \n",
       "word 2        profit                  0.032615         fuel   \n",
       "word 3        growth                  0.026105       German   \n",
       "word 4          gets                  0.026105          BMW   \n",
       "word 5         sales                  0.019595        trade   \n",
       "word 6          hits                  0.013085         rise   \n",
       "word 7          2004                  0.013085       French   \n",
       "word 8          loan                  0.013085       record   \n",
       "word 9    production                  0.013085       prices   \n",
       "word 10         high                  0.013085        boost   \n",
       "\n",
       "         topic-1-word-probability topic-2-word  topic-2-word-probability  \n",
       "word 1                   0.042143         jobs                  0.024510  \n",
       "word 2                   0.035131      profits                  0.024510  \n",
       "word 3                   0.021107        Yukos                  0.024510  \n",
       "word 4                   0.021107         firm                  0.024510  \n",
       "word 5                   0.014094        Japan                  0.024510  \n",
       "word 6                   0.014094        India                  0.018397  \n",
       "word 7                   0.014094         hits                  0.018397  \n",
       "word 8                   0.014094          new                  0.018397  \n",
       "word 9                   0.014094     Japanese                  0.018397  \n",
       "word 10                  0.014094       $280bn                  0.012285  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(config[\"integer-word-dict-path\"], 'r') as file:\n",
    "    revdictionary = json.load(file)\n",
    "topic_word_distribution = lda.getWordsPerTopic(revdictionary)\n",
    "\n",
    "twd = {}\n",
    "for topic in topic_word_distribution.keys():\n",
    "    words, probabilities = zip(*topic_word_distribution[topic])\n",
    "    twd[topic + \"-word\"] = words\n",
    "    twd[topic + \"-word-probability\"] = probabilities\n",
    "\n",
    "df_twd = pd.DataFrame(twd, index = [\"word \" +str(i+1) for i in range(len(topic_word_distribution['topic-1']))])\n",
    "df_twd.to_csv(\"data/output-data/topic-word-distribution.tsv\", sep=\"\\t\", index = False)\n",
    "df_twd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cc7c59-b386-40be-b1b5-0b361fdda0a1",
   "metadata": {},
   "source": [
    "*Document topic distribution*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d50a05-c34c-45f0-9ec4-a38a65a7651d",
   "metadata": {},
   "source": [
    "*print all details:*\n",
    "- Integer encoded dataset\n",
    "- Final state of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31e55f81-a968-4fd2-8f55-7e4dca76564c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topics:  3\n",
      "dataset:  [[0, 1, 2, 3, 4], [5, 6, 7, 8, 9], [10, 11, 12, 13, 14], [15, 16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 6], [27, 28, 29, 30, 31], [32, 33, 34, 35, 36], [3, 37, 7, 38, 39], [0, 40, 41, 42], [43, 12, 44, 45, 46, 47], [48, 49, 50, 51], [52, 53, 54, 55, 56], [57, 58, 59, 60, 61, 62], [63, 64, 65, 66, 67], [68, 69, 70, 71], [72, 73, 74, 75, 76], [77, 78, 7, 79, 80], [57, 81, 82, 83], [84, 85, 86, 87], [88, 89, 90, 47], [91, 92, 93, 94], [95, 96, 97, 98, 99, 9], [100, 101, 102, 103], [104, 105, 106, 9], [107, 108, 109, 110, 111], [112, 113, 114, 115, 116], [117, 118, 119, 120, 121], [122, 96, 123, 56], [124, 125, 56, 126], [127, 85, 128, 129], [130, 131, 132, 133, 134], [135, 115, 136, 137, 138], [26, 131, 139, 140, 141, 142], [143, 6, 144, 145], [146, 147, 148, 149], [150, 151, 152, 153, 49], [154, 155, 139, 156, 53], [127, 157, 83, 158], [159, 160, 161, 162], [163, 12, 164, 165], [166, 167, 168, 169], [170, 171, 172, 173], [174, 175, 176, 177, 178], [179, 47, 180, 181, 39], [182, 183, 184, 185, 94], [186, 187, 75, 0, 188], [189, 190, 191, 100], [192, 193, 194, 195, 196], [197, 198, 199, 49], [200, 44, 201, 202], [203, 204, 66, 205, 206], [207, 208, 209, 210, 211, 212], [213, 214, 215, 216], [57, 217, 218, 219], [220, 221, 222, 223], [224, 225, 226, 227, 228], [182, 229, 12, 230, 70], [100, 231, 232, 233], [96, 234, 47, 201, 235], [236, 49, 237, 238, 239, 240], [241, 242, 243], [100, 6, 244, 103], [245, 61, 246, 247], [248, 249, 139, 64, 250], [127, 251, 252, 173], [182, 253, 38, 52, 254], [255, 139, 256, 257], [258, 259, 260, 261], [262, 263, 264], [265, 266, 80, 56, 267], [268, 269, 270, 271], [272, 273, 274], [275, 276, 277, 278, 279], [280, 281, 282, 190, 52, 267], [192, 283, 284, 56], [285, 14, 286, 287], [288, 289, 290, 291, 292], [293, 294, 7, 295], [296, 297, 298, 299], [300, 301, 302, 303, 304], [96, 60, 305, 306, 307], [308, 7, 309, 85], [310, 311, 312, 313], [314, 292, 77, 315], [310, 316, 317, 318, 319], [6, 320, 321, 322, 85], [88, 323, 324], [325, 326, 327, 328], [329, 330, 331, 332], [333, 334, 335, 294], [336, 337, 115, 49], [338, 6, 339], [340, 341, 342, 12, 343], [8, 344, 345, 244], [346, 75, 17, 347, 348], [349, 350, 333, 9], [117, 351, 57, 352], [353, 354, 355, 356, 49], [357, 358, 359, 360, 8]]\n",
      "dataset size:  100\n",
      "vocab:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360]\n",
      "vocab size:  361\n",
      "ndt:  [[0 3 2]\n",
      " [2 1 2]\n",
      " [0 5 0]\n",
      " [2 1 2]\n",
      " [4 0 0]\n",
      " [2 1 1]\n",
      " [0 2 3]\n",
      " [1 3 1]\n",
      " [2 2 1]\n",
      " [0 0 4]\n",
      " [0 5 1]\n",
      " [2 1 1]\n",
      " [3 2 0]\n",
      " [3 0 3]\n",
      " [0 0 5]\n",
      " [4 0 0]\n",
      " [2 1 2]\n",
      " [2 1 2]\n",
      " [0 1 3]\n",
      " [3 0 1]\n",
      " [0 2 2]\n",
      " [0 4 0]\n",
      " [1 0 5]\n",
      " [0 0 4]\n",
      " [0 0 4]\n",
      " [3 1 1]\n",
      " [3 1 1]\n",
      " [2 0 3]\n",
      " [1 0 3]\n",
      " [3 0 1]\n",
      " [2 1 1]\n",
      " [2 2 1]\n",
      " [5 0 0]\n",
      " [4 2 0]\n",
      " [1 3 0]\n",
      " [0 3 1]\n",
      " [2 3 0]\n",
      " [1 3 1]\n",
      " [0 0 4]\n",
      " [1 3 0]\n",
      " [0 3 1]\n",
      " [1 2 1]\n",
      " [0 0 4]\n",
      " [2 2 1]\n",
      " [3 1 1]\n",
      " [0 3 2]\n",
      " [1 0 4]\n",
      " [0 1 3]\n",
      " [2 0 3]\n",
      " [0 1 3]\n",
      " [1 3 0]\n",
      " [0 3 2]\n",
      " [1 3 2]\n",
      " [4 0 0]\n",
      " [0 0 4]\n",
      " [3 1 0]\n",
      " [3 0 2]\n",
      " [1 4 0]\n",
      " [1 1 2]\n",
      " [0 2 3]\n",
      " [2 4 0]\n",
      " [0 2 1]\n",
      " [1 1 2]\n",
      " [3 1 0]\n",
      " [3 0 2]\n",
      " [1 0 3]\n",
      " [2 3 0]\n",
      " [1 0 3]\n",
      " [0 1 3]\n",
      " [2 0 1]\n",
      " [4 0 1]\n",
      " [0 0 4]\n",
      " [0 0 3]\n",
      " [4 1 0]\n",
      " [2 0 4]\n",
      " [2 0 2]\n",
      " [0 4 0]\n",
      " [3 2 0]\n",
      " [4 0 0]\n",
      " [3 1 0]\n",
      " [2 1 2]\n",
      " [0 2 3]\n",
      " [2 1 1]\n",
      " [2 2 0]\n",
      " [2 1 1]\n",
      " [2 2 1]\n",
      " [4 0 1]\n",
      " [0 2 1]\n",
      " [2 0 2]\n",
      " [1 3 0]\n",
      " [3 0 1]\n",
      " [0 3 1]\n",
      " [2 1 0]\n",
      " [1 4 0]\n",
      " [0 4 0]\n",
      " [1 0 4]\n",
      " [1 0 3]\n",
      " [1 1 2]\n",
      " [1 4 0]\n",
      " [0 1 4]]\n",
      "ndsum:  [5 5 5 5 4 4 5 5 5 4 6 4 5 6 5 4 5 5 4 4 4 4 6 4 4 5 5 5 4 4 4 5 5 6 4 4 5\n",
      " 5 4 4 4 4 4 5 5 5 5 4 5 4 4 5 6 4 4 4 5 5 4 5 6 3 4 4 5 4 5 4 4 3 5 4 3 5\n",
      " 6 4 4 5 4 4 5 5 4 4 4 5 5 3 4 4 4 4 3 5 4 5 4 4 5 5]\n",
      "ntw:  [[0 0 0 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]\n",
      " [3 0 1 ... 1 1 1]]\n",
      "ntsum:  [150 139 160]\n",
      "z:  [[2, 1, 2, 1, 1], [0, 0, 2, 1, 2], [1, 1, 1, 1, 1], [0, 0, 2, 2, 1], [0, 0, 0, 0], [2, 0, 1, 0], [2, 1, 2, 1, 2], [0, 2, 1, 1, 1], [1, 0, 2, 1, 0], [2, 2, 2, 2], [1, 1, 1, 1, 1, 2], [0, 1, 2, 0], [0, 1, 0, 1, 0], [2, 0, 0, 2, 0, 2], [2, 2, 2, 2, 2], [0, 0, 0, 0], [2, 0, 0, 2, 1], [1, 2, 2, 0, 0], [2, 2, 1, 2], [0, 0, 2, 0], [1, 2, 1, 2], [1, 1, 1, 1], [2, 2, 0, 2, 2, 2], [2, 2, 2, 2], [2, 2, 2, 2], [0, 0, 2, 0, 1], [0, 1, 2, 0, 0], [0, 0, 2, 2, 2], [2, 2, 2, 0], [0, 2, 0, 0], [2, 0, 0, 1], [0, 1, 1, 2, 0], [0, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0], [1, 0, 1, 1], [1, 1, 2, 1], [0, 1, 1, 0, 1], [1, 2, 0, 1, 1], [2, 2, 2, 2], [1, 1, 1, 0], [2, 1, 1, 1], [0, 1, 1, 2], [2, 2, 2, 2], [0, 0, 1, 2, 1], [0, 2, 1, 0, 0], [1, 1, 2, 2, 1], [2, 0, 2, 2, 2], [1, 2, 2, 2], [0, 2, 2, 0, 2], [2, 2, 2, 1], [0, 1, 1, 1], [2, 1, 2, 1, 1], [1, 2, 2, 1, 0, 1], [0, 0, 0, 0], [2, 2, 2, 2], [0, 0, 1, 0], [0, 2, 0, 2, 0], [1, 1, 1, 1, 0], [2, 2, 0, 1], [2, 1, 2, 1, 2], [1, 1, 1, 0, 0, 1], [1, 2, 1], [2, 0, 1, 2], [0, 0, 0, 1], [2, 0, 0, 0, 2], [2, 0, 2, 2], [1, 1, 1, 0, 0], [2, 0, 2, 2], [2, 1, 2, 2], [0, 2, 0], [2, 0, 0, 0, 0], [2, 2, 2, 2], [2, 2, 2], [0, 0, 1, 0, 0], [2, 2, 2, 2, 0, 0], [0, 2, 2, 0], [1, 1, 1, 1], [1, 0, 1, 0, 0], [0, 0, 0, 0], [0, 0, 0, 1], [2, 2, 0, 0, 1], [2, 2, 1, 1, 2], [2, 0, 1, 0], [1, 1, 0, 0], [0, 0, 1, 2], [1, 0, 0, 2, 1], [0, 0, 2, 0, 0], [1, 2, 1], [2, 0, 0, 2], [1, 1, 0, 1], [0, 2, 0, 0], [2, 1, 1, 1], [0, 0, 1], [1, 1, 0, 1, 1], [1, 1, 1, 1], [2, 2, 2, 2, 0], [2, 2, 0, 2], [0, 1, 2, 2], [1, 1, 0, 1, 1], [2, 2, 2, 2, 1]]\n"
     ]
    }
   ],
   "source": [
    "  # prints everything - for debugging\n",
    "lda.printall()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
